{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Training a convolutional neural network on CIFAR-10\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you shall build a convolutional neural network and play with **batch normlization**, **dropout**, **regularization** and **augmentation** to imporve accuracy on the Cifar-10 dataset.\n",
    "\n",
    "**Important!**\n",
    "You will need to add code at locations indicated with \"ToDo\" only for the program to run. However, feel free to change what you like.\n",
    "\n",
    "**Note:**\n",
    "If you want to use the \"ML servers\" remember to change kernal to python 3.6. (Kernal->Change kernal->python 3.6(Conda)). Do the programming and the initial tests on the CPU, then select a GPU for effective traning. To select a particular GPU, use the keys \"cuda\"->\"device\" within the config dictionary.\n",
    "\n",
    "Software verion:\n",
    "- Python 3.6\n",
    "- Pytorch 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "## Step 0: Check CPU, memory and GPU usage on the ML server\n",
    "\n",
    "If you use one of the ML servers and what to know the resorese being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using_ML_sever = True\n",
    "if using_ML_sever:\n",
    "    from IPython.display import Image, display\n",
    "    display(Image('/tmp/gpu.png'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Configuration\n",
    "---\n",
    "To keep track of important parameters, we use dictionary \"config\". You should play around with the values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          'batch_size':64,          # Training batch size\n",
    "          'cuda': {'use_cuda':True,  # Use_cuda=True: use GPU\n",
    "                   'device_idx': 0}, # Select gpu index: 0,1,2,3\n",
    "          'log_interval':20,         # How often to dislay (batch) loss during training\n",
    "          'epochs': 50,              # Number of epochs\n",
    "          'learningRate': 0.001,     # learning rate to the optimizer\n",
    "          'momentum': 0.9,            # momentum in the SGD optimizer\n",
    "          'use_augmentation': True,  # Use augmentation\n",
    "          'weight_decay': 0.0001     # weight_decay value\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: The Cifar-10 dataset\n",
    "\n",
    "Torchvision is a pytorch package which consists of popular datasets, model architectures, and common image transformations for computer vision. Torchvision includes a \"dataloader\" for the Cifar-10 dataset which we will use. We will also use torchvision's \"transforms\" module to perform augmentation and normalization. \n",
    "\n",
    "\n",
    "The Cifar-10 dataset have 10 classes: ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'].  \n",
    "\n",
    "The training set consists of 50,000 images and the test set consists of 10,000 images. The images are of size [3,32,32].\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The output of torchvision datasets are PILImage images of range [0, 1]. \n",
    "#We transform them to Tensors of normalized range [-1, 1].\n",
    "# Data\n",
    "\n",
    "#train transforms\n",
    "train_transform_list = []\n",
    "if config['use_augmentation']:\n",
    "    train_transform_list.append(transforms.RandomCrop(32, padding=4))\n",
    "    train_transform_list.append(transforms.RandomHorizontalFlip())\n",
    "train_transform_list.append(transforms.ToTensor()) \n",
    "train_transform_list.append(transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)))\n",
    "transform_train = transforms.Compose(train_transform_list)\n",
    "\n",
    "#test transforms\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "#Path to the Cifar-10 dataset\n",
    "dataPath = './data/Cifar-10/'\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = datasets.CIFAR10(root=dataPath, train=True, download=True, transform=transform_train)\n",
    "val_dataset   = datasets.CIFAR10(root=dataPath, train=False, download=True, transform=transform_val)\n",
    "\n",
    "# Create dataLoaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=2)\n",
    "val_loader   = torch.utils.data.DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "plt.figure(figsize=(18, 16), dpi=80)\n",
    "labels = np.array([x[1] for x in val_dataset])\n",
    "for y, cls in enumerate(classes):              \n",
    "    idxs = np.flatnonzero(labels == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        img = (val_dataset[idx][0]*0.2 + 0.5)*255\n",
    "        img = img.permute(1, 2, 0).numpy()\n",
    "        img = np.minimum(img, 255)\n",
    "        plt.imshow(img.astype(np.uint8))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Build the model\n",
    "\n",
    "The input has shape [batch size, 3,32,32]. Use what we have learnt to build a convultional neural network. Have a look at the useful classes:\n",
    "\n",
    "- nn.Conv2D\n",
    "- nn.MaxPool2d\n",
    "- nn.Linear\n",
    "- nn.BatchNorm2d\n",
    "- nn.Dropout2d\n",
    "\n",
    "\n",
    "\n",
    "Note that the model inherits from \"torch.nn.Module\", which requires the two class methods \"__init__\" and \"forward\". As discussed in the lecture, the former defines the layers used by the model, while the latter defines how the layers are stacked inside the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        #ToDO\n",
    "\n",
    "    def forward(self, x):\n",
    "        #ToDO       \n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creat an instance of Model\n",
    "model = Model()\n",
    "if config['cuda']['use_cuda']:\n",
    "    model.to(f'cuda:{config[\"cuda\"][\"device_idx\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Define optimizer and loss function\n",
    "\n",
    "Instantiate an optimizer, e.g. stochastic gradient descent, from the \"torch.optim\" module (https://pytorch.org/docs/stable/optim.html) with your model. Remember that we have defined \"learning rate\" inside the config-dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of \"torch.optim.SGD\"\n",
    "\n",
    "#optimizer = optim.SGD(model.parameters(), lr=config['learningRate'], momentum=config['momentum'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=config['learningRate'], weight_decay=config['weight_decay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here we want to define the loss function (often called criterion). As we are dealing with a classification problem, you should use the softmax cross entropy loss.\n",
    "\n",
    "Hint, have a look here: (https://pytorch.org/docs/stable/nn.html#torch-nn-functional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(prediction, labels):\n",
    "    \"\"\"Returns softmax cross entropy loss.\"\"\"\n",
    "    loss = F.cross_entropy(input=prediction, target=labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5: Set up the training process and train the model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, epoch, data_loader, optimizer, is_training, config):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model        (obj): The neural network model\n",
    "        epoch        (int): The current epoch\n",
    "        data_loader  (obj): A pytorch data loader \"torch.utils.data.DataLoader\"\n",
    "        optimizer    (obj): A pytorch optimizer \"torch.optim\"\n",
    "        is_training (bool): Whether to use train (update) the model/weights or not. \n",
    "        config      (dict): Configuration parameters\n",
    "\n",
    "    Intermediate:\n",
    "        totalLoss: (float): The accumulated loss from all batches. \n",
    "                            Hint: Should be a numpy scalar and not a pytorch scalar\n",
    "\n",
    "    Returns:\n",
    "        loss_avg         (float): The average loss of the dataset\n",
    "        accuracy         (float): The average accuracy of the dataset\n",
    "        confusion_matrix (float): A 10x10 matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_training==True: \n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss        = 0 \n",
    "    correct          = 0 \n",
    "    confusion_matrix = np.zeros(shape=(10,10))\n",
    "    labels_list      = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(data_loader):\n",
    "        if config['cuda']['use_cuda']:\n",
    "            images = data_batch[0].to(f'cuda:{config[\"cuda\"][\"device_idx\"]}') # send data to GPU\n",
    "            labels = data_batch[1].to(f'cuda:{config[\"cuda\"][\"device_idx\"]}') # send data to GPU\n",
    "        else:\n",
    "            images = data_batch[0]\n",
    "            labels = data_batch[1]\n",
    "\n",
    "        if not is_training:\n",
    "            with torch.no_grad():\n",
    "                prediction = model.forward(images)\n",
    "                # Note: It can be beneficial to detach \"total_loss\" from the graph, consider convert \"total_loss\" to numpy.\n",
    "                loss        = loss_fn(prediction, labels)\n",
    "                total_loss += loss.item()    \n",
    "            \n",
    "        elif is_training: \n",
    "            prediction = model.forward(images)\n",
    "            # Note: It can be beneficial to detach \"total_loss\" from the graph, consider convert \"total_loss\" to numpy.\n",
    "            loss        = loss_fn(prediction, labels)\n",
    "            total_loss += loss.item()  \n",
    "\n",
    "            # take a gradient update\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Compute the correct classification\n",
    "        predicted_label  = prediction.max(1, keepdim=True)[1][:,0]\n",
    "        correct          += predicted_label.eq(labels).cpu().sum().numpy()\n",
    "        confusion_matrix += metrics.confusion_matrix(labels.cpu().numpy(), predicted_label.cpu().numpy(), labels_list)\n",
    "\n",
    "        # Print statistics\n",
    "        batchSize = len(labels)\n",
    "        if batch_idx % config['log_interval'] == 0:\n",
    "            print(f'Epoch={epoch} | {batch_idx/len(data_loader)*100:.2f}% | loss = {loss/batchSize:.5f}')\n",
    "\n",
    "    loss_avg         = total_loss / len(data_loader)\n",
    "    accuracy         = correct / len(data_loader.dataset)\n",
    "    confusion_matrix = confusion_matrix / len(data_loader.dataset)\n",
    "\n",
    "    return loss_avg, accuracy, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here is where the action takes place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "train_loss = np.zeros(shape=config['epochs'])\n",
    "train_acc  = np.zeros(shape=config['epochs'])\n",
    "val_loss   = np.zeros(shape=config['epochs'])\n",
    "val_acc    = np.zeros(shape=config['epochs'])\n",
    "val_confusion_matrix   = np.zeros(shape=(10,10,config['epochs']))\n",
    "train_confusion_matrix = np.zeros(shape=(10,10,config['epochs']))\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    train_loss[epoch], train_acc[epoch], train_confusion_matrix[:,:,epoch] = \\\n",
    "                               run_epoch(model, epoch, train_loader, optimizer, is_training=True, config=config)\n",
    "\n",
    "    val_loss[epoch], val_acc[epoch], val_confusion_matrix[:,:,epoch]     = \\\n",
    "                               run_epoch(model, epoch, val_loader, optimizer, is_training=False, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 6. Show results\n",
    "Plot the loss and the accuracy as a function of epochs to monitor the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training accuracy and the training loss\n",
    "#plt.figure()\n",
    "plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "# plt.subplots_adjust(hspace=2)\n",
    "ax.plot(train_loss, 'b', label='train loss')\n",
    "ax.plot(val_loss, 'r', label='validation loss')\n",
    "ax.grid()\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "ax.legend(loc='upper right', fontsize=16)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "ax.plot(train_acc, 'b', label='train_acc')\n",
    "ax.plot(val_acc, 'r', label='validation accuracy')\n",
    "ax.grid()\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "plt.xlabel('Iterations', fontsize=18)\n",
    "val_acc_max = np.max(val_acc)\n",
    "val_acc_max_ind = np.argmax(val_acc)\n",
    "plt.axvline(x=val_acc_max_ind, color='g', linestyle='--', label='Highest validation accuracy')\n",
    "plt.title('Highest validation accuracy = %0.1f %%' % (val_acc_max*100), fontsize=16)\n",
    "ax.legend(loc='lower right', fontsize=16)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let us study the accuracy per class on the validation dataset. We use the result from the epoch with highest validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmax(val_acc)\n",
    "class_accuracy = val_confusion_matrix[:,:,ind]\n",
    "for ii in range(len(classes)):\n",
    "    acc = val_confusion_matrix[ii,ii,ind] / np.sum(val_confusion_matrix[ii,:,ind])\n",
    "    print(f'Accuracy of {str(classes[ii]).ljust(15)}: {acc*100:.01f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In order to see how the network learns to distinguish the different classes as the training progresses we can plot the confusion matrices as heatmaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "epoch_step                  = 2    \n",
    "set_colorbar_max_percentage = 10 \n",
    "    \n",
    "# Plot confusion matrices\n",
    "ticks = np.linspace(0,9,10)\n",
    "gridspec_kwargs = dict(top=0.9, bottom=0.1, left=0.0, right=0.9, wspace=0.5, hspace=0.2)\n",
    "for i in range(0, config['epochs'], epoch_step):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 16), gridspec_kw=gridspec_kwargs)\n",
    "    im = ax1.imshow(val_confusion_matrix[:, :, i]*100)\n",
    "    ax1.set_title(f'Validation: Epoch #{i}', fontsize=18)\n",
    "    ax1.set_xticks(ticks=ticks)\n",
    "    ax1.set_yticks(ticks=ticks)\n",
    "    ax1.set_yticklabels(classes)\n",
    "    im.set_clim(0.0, set_colorbar_max_percentage)\n",
    "    ax1.set_xticklabels(classes, rotation=45)\n",
    "    ax1.set_ylabel('Prediction', fontsize=16)\n",
    "    ax1.set_xlabel('Groundtruth', fontsize=16)\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax     = divider.append_axes('right', size='5%', pad=0.15)\n",
    "    f.colorbar(im, cax=cax, orientation='vertical')\n",
    "    \n",
    "    im = ax2.imshow(train_confusion_matrix[:, :, i]*100)\n",
    "    ax2.set_title(f'Train: Epoch #{i}', fontsize=18)\n",
    "    ax2.set_xticks(ticks=ticks)\n",
    "    ax2.set_yticks(ticks=ticks)\n",
    "    ax2.set_yticklabels(classes)\n",
    "    im.set_clim(0.0, set_colorbar_max_percentage)\n",
    "    ax2.set_xticklabels(classes, rotation=45)\n",
    "    ax2.set_ylabel('Prediction', fontsize=16)\n",
    "    ax2.set_xlabel('Ground truth', fontsize=16)\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax     = divider.append_axes('right', size='5%', pad=0.15)\n",
    "    f.colorbar(im, cax=cax, orientation='vertical')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.6 (Conda)",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
