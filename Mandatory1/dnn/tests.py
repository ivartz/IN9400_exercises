#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::#
#                                                                               #
# Part of mandatory assignment 1 in                                             #
# IN5400 - Machine Learning for Image analysis                                  #
# University of Oslo                                                            #
#                                                                               #
#                                                                               #
# Ole-Johan Skrede    olejohas at ifi dot uio dot no                            #
# 2019.02.12                                                                    #
#                                                                               #
#:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::#

"""
In this file, resources for testing the implementation is provided.
"""

import numpy as np

def task_2a():
    """Input and corresponding output for task 2 a)"""

    Z = np.array([[-3.0,  0.00,  3.00],
                  [-3.0,  1.00,  5.00],
                  [-4.5,  0.12, -0.12]])

    A = np.array([[0.0, 0.0, 3.0],
                  [0.0, 1.0, 5.0],
                  [0.0, 0.12, 0.0]])
    return Z, A

def task_2b():
    """Input and corresponding output for task 2 b)"""

    Z = np.array([[-3.00,  0.00,  3.00],
                  [-3.00,  1.00,  5.00],
                  [-4.50,  0.12, -0.12]])
    S = np.array([[0.44981622, 0.20636518, 0.11857876],
                  [0.44981622, 0.56095872, 0.87618513],
                  [0.10036756, 0.23267609, 0.00523610]])
    return Z, S

def task_2c():
    """Input and corresponding output for task 2 c)"""
    # We create a simple network with 3 input nodes, one hidden layer with 5 nodes, and an output
    # layer with 4 nodes
    conf = {"layer_dimensions": [3, 5, 4],
            "activation_function": "relu"}

    # We create a batch with 6 examples, each with only 3 entries
    X_batch = np.array([[ 1,  2, -5,  4,  5, -2],
                        [-2, -3,  5,  3, -1,  0],
                        [-4,  3,  3,  4,  2,  3]])

    # We create some arbitrary parameters with the correct shape
    params = {"W_1": np.array([[ 0.1,  0.2,  0.3,  0.4,  0.1],
                               [-0.1,  0.2, -0.3,  0.4,  0.2],
                               [ 0.0,  0.0,  0.1,  0.2, -0.2]]),
              "W_2": np.array([[ 0.3, -0.2,  0.0,  0.2],
                               [ 0.3, -0.1, -0.2,  0.1],
                               [ 0.4,  0.2,  0.1,  0.2],
                               [-0.1,  0.0,  0.0, -0.2],
                               [-0.1,  0.0,  0.0, -0.2]]),
              "b_1": np.array([[ 0.2],
                               [-0.2],
                               [ 0.1],
                               [-0.4],
                               [-0.2]]),
              "b_2": np.array([[ 0.4],
                               [-0.2],
                               [-0.4],
                               [ 0.5]])
             }

    # We expect features to have the following entries: ['A_0', 'A_1', 'A_2', 'Z_1', 'Z_2'], where
    # features['A_0'] is the input and features['A_2'] is the predicted softmax output.

    ## Z_1: Linear activation in the first layer
    Z_1 = np.array([[ 0.5,  0.7, -0.8,  0.3,  0.8,  0.0],
                    [-0.4, -0.4, -0.2,  1.2,  0.6, -0.6],
                    [ 0.6,  1.9, -2.6,  0.8,  2.1, -0.2],
                    [-1.6, -0.2,  0.2,  3.2,  1.6, -0.6],
                    [ 0.3, -1.2, -0.3,  0.0, -0.3, -1.0]])

    ## A_1: ReLu(Z_1)
    A_1 = np.array([[ 0.5,  0.7,  0.0,  0.3,  0.8,  0.0],
                    [ 0.0,  0.0,  0.0,  1.2,  0.6,  0.0],
                    [ 0.6,  1.9,  0.0,  0.8,  2.1,  0.0],
                    [ 0.0,  0.0,  0.2,  3.2,  1.6,  0.0],
                    [ 0.3,  0.0,  0.0,  0.0,  0.0,  0.0]])

    ## Z_2: Linear activation in the output layer, also called "logits"
    Z_2 = np.array([[ 0.76,  1.37,  0.38,  0.85,  1.50,  0.40],
                    [-0.18,  0.04, -0.20, -0.22,  0.00, -0.20],
                    [-0.34, -0.21, -0.40, -0.56, -0.31, -0.40],
                    [ 0.66,  1.02,  0.46,  0.20,  0.82,  0.50]])

    ## A_2: softmax(Z_2), the prediction of the network for all examples in the batch
    Y = np.array([[0.38046881, 0.45974043, 0.32241513, 0.47411395, 0.52815006, 0.32223642],
                  [0.14862171, 0.12159089, 0.18051970, 0.16262512, 0.11784621, 0.17684710],
                  [0.12664707, 0.09469508, 0.14779703, 0.11575174, 0.08643394, 0.14479016],
                  [0.34426242, 0.32397360, 0.34926814, 0.24750919, 0.26756979, 0.35612632]])

    return conf, X_batch, params, Z_1, A_1, Z_2, Y

def task_3():
    """Input and corresponding output for task 3"""

    # We re-use the expected output from the previous task, and create a dummy reference output
    Y = np.array([[0.38046881, 0.45974043, 0.32241513, 0.47411395, 0.52815006, 0.32223642],
                  [0.14862171, 0.12159089, 0.18051970, 0.16262512, 0.11784621, 0.17684710],
                  [0.12664707, 0.09469508, 0.14779703, 0.11575174, 0.08643394, 0.14479016],
                  [0.34426242, 0.32397360, 0.34926814, 0.24750919, 0.26756979, 0.35612632]])
    Y_batch = np.array([[1.0, 0.0, 0.0, 0.0, 1.0, 0.0],
                        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],
                        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],
                        [0.0, 0.0, 1.0, 0.0, 0.0, 1.0]])

    cost_value = 1.325418635486801
    num_correct = 4

    return Y, Y_batch, cost_value, num_correct

def task_4a():
    """Input and corresponding output for task 4 a)"""

    # Create some dummy input
    Z = np.array([[-3.02,  0.12,  3.01],
                  [-3.04, -1.05,  5.03],
                  [-4.55,  0.12, -0.12]])

    # Create expected output
    dg_dz = np.array([[0.0, 1.0, 1.0],
                      [0.0, 0.0, 1.0],
                      [0.0, 1.0, 0.0]])

    return Z, dg_dz

def task_4b():
    """Input and corresponding output for task 4 b)"""

    # We will use the same network architecture as we used in Task 2
    conf = {"layer_dimensions": [3, 5, 4],
            "activation_function": "relu"}

    # For a reference and proposed output, we will use the same values as we did in Task 3
    Y = np.array([[0.38046881, 0.45974043, 0.32241513, 0.47411395, 0.52815006, 0.32223642],
                  [0.14862171, 0.12159089, 0.18051970, 0.16262512, 0.11784621, 0.17684710],
                  [0.12664707, 0.09469508, 0.14779703, 0.11575174, 0.08643394, 0.14479016],
                  [0.34426242, 0.32397360, 0.34926814, 0.24750919, 0.26756979, 0.35612632]])
    Y_batch = np.array([[1.0, 0.0, 0.0, 0.0, 1.0, 0.0],
                        [0.0, 1.0, 0.0, 0.0, 0.0, 0.0],
                        [0.0, 0.0, 0.0, 1.0, 0.0, 0.0],
                        [0.0, 0.0, 1.0, 0.0, 0.0, 1.0]])

    # This is the same parameters as we used in Task 2
    params = {"W_1": np.array([[ 0.1,  0.2,  0.3,  0.4,  0.1],
                               [-0.1,  0.2, -0.3,  0.4,  0.2],
                               [ 0.0,  0.0,  0.1,  0.2, -0.2]]),
              "W_2": np.array([[ 0.3, -0.2,  0.0,  0.2],
                               [ 0.3, -0.1, -0.2,  0.1],
                               [ 0.4,  0.2,  0.1,  0.2],
                               [-0.1,  0.0,  0.0, -0.2],
                               [-0.1,  0.0,  0.0, -0.2]]),
              "b_1": np.array([[ 0.2],
                               [-0.2],
                               [ 0.1],
                               [-0.4],
                               [-0.2]]),
              "b_2": np.array([[ 0.4],
                               [-0.2],
                               [-0.4],
                               [ 0.5]])
             }

    # For feature arrays, we use the expected result from Task 2, where we remember that A_0 is the
    # input and A_2 the proposed output.
    features = {'A_0': np.array([[ 1,  2, -5,  4,  5, -2],
                                 [-2, -3,  5,  3, -1,  0],
                                 [-4,  3,  3,  4,  2,  3]]),
        'Z_1': np.array([[ 0.5,  0.7, -0.8,  0.3,  0.8,  0.0],
                         [-0.4, -0.4, -0.2,  1.2,  0.6, -0.6],
                         [ 0.6,  1.9, -2.6,  0.8,  2.1, -0.2],
                         [-1.6, -0.2,  0.2,  3.2,  1.6, -0.6],
                         [ 0.3, -1.2, -0.3,  0.0, -0.3, -1.0]]),
        'A_1': np.array([[ 0.5,  0.7,  0.0,  0.3,  0.8,  0.0],
                         [ 0.0,  0.0,  0.0,  1.2,  0.6,  0.0],
                         [ 0.6,  1.9,  0.0,  0.8,  2.1,  0.0],
                         [ 0.0,  0.0,  0.2,  3.2,  1.6,  0.0],
                         [ 0.3,  0.0,  0.0,  0.0,  0.0,  0.0]]),
        'Z_2': np.array([[ 0.76,  1.37,  0.38,  0.85,  1.50,  0.40],
                         [-0.18,  0.04, -0.20, -0.22,  0.00, -0.20],
                         [-0.34, -0.21, -0.40, -0.56, -0.31, -0.40],
                         [ 0.66,  1.02,  0.46,  0.20,  0.82,  0.50]]),
        'A_2': np.array([[0.38046881, 0.45974043, 0.32241513, 0.47411395, 0.52815006, 0.32223642],
                         [0.14862171, 0.12159089, 0.18051970, 0.16262512, 0.11784621, 0.17684710],
                         [0.12664707, 0.09469508, 0.14779703, 0.11575174, 0.08643394, 0.14479016],
                         [0.34426242, 0.32397360, 0.34926814, 0.24750919, 0.26756979, 0.35612632]])
        }


    # The gradient of the cost function with respect to W_1
    grad_W_1 = np.array([[ 0.13730088,  0.09849032,  0.04105163, -0.15147034, -0.06575872],
                         [-0.04208172,  0.18776436,  0.11307706,  0.03418559, -0.04615683],
                         [ 0.32222077,  0.17042503,  0.22011387, -0.01776605, -0.06000925]])

    # The gradient of the cost function with respect to b_1
    grad_b_1 = np.array([[ 0.03529916],
                         [ 0.03061714],
                         [ 0.00435698],
                         [-0.00088956],
                         [-0.01730210]])

    # The gradient of the cost function with respect to W_2
    grad_W_2 = np.array([[-0.03719884, -0.06625184, -0.01108621,  0.11453689],
                         [ 0.0476378 ,  0.04430964, -0.16820626,  0.07625882],
                         [-0.01830094, -0.20037119, -0.04499641,  0.26366853],
                         [ 0.13778129,  0.12417638, -0.44362345,  0.18166578],
                         [-0.03097656,  0.00743109,  0.00633235,  0.01721312]])


    # The gradient of the cost function with respect to b_2
    grad_b_2 = np.array([[ 0.08118747],
                         [-0.01532488],
                         [-0.04731416],
                         [-0.01854842]])

    return conf, Y, Y_batch, params, features, grad_W_1, grad_b_1, grad_W_2, grad_b_2

def task_5():
    """Input and corresponding output for task 5"""

    # We set the learning rate to 0.02
    conf = {'learning_rate': 0.02}

    # And we re-use the parameters and their gradients from Task 4
    params = {"W_1": np.array([[ 0.1,  0.2,  0.3,  0.4,  0.1],
                               [-0.1,  0.2, -0.3,  0.4,  0.2],
                               [ 0.0,  0.0,  0.1,  0.2, -0.2]]),
              "W_2": np.array([[ 0.3, -0.2,  0.0,  0.2],
                               [ 0.3, -0.1, -0.2,  0.1],
                               [ 0.4,  0.2,  0.1,  0.2],
                               [-0.1,  0.0,  0.0, -0.2],
                               [-0.1,  0.0,  0.0, -0.2]]),
              "b_1": np.array([[ 0.2],
                               [-0.2],
                               [ 0.1],
                               [-0.4],
                               [-0.2]]),
              "b_2": np.array([[ 0.4],
                               [-0.2],
                               [-0.4],
                               [ 0.5]])
             }
    grad_params = {
        'grad_W_1': np.array([[ 0.13730088,  0.09849032,  0.04105163, -0.15147034, -0.06575872],
                              [-0.04208172,  0.18776436,  0.11307706,  0.03418559, -0.04615683],
                              [ 0.32222077,  0.17042503,  0.22011387, -0.01776605, -0.06000925]]),
        'grad_b_1': np.array([[ 0.03529916],
                              [ 0.03061714],
                              [ 0.00435698],
                              [-0.00088956],
                              [-0.01730210]]),
        'grad_W_2': np.array([[-0.03719884, -0.06625184, -0.01108621,  0.11453689],
                              [ 0.0476378 ,  0.04430964, -0.16820626,  0.07625882],
                              [-0.01830094, -0.20037119, -0.04499641,  0.26366853],
                              [ 0.13778129,  0.12417638, -0.44362345,  0.18166578],
                              [-0.03097656,  0.00743109,  0.00633235,  0.01721312]]),
        'grad_b_2': np.array([[ 0.08118747],
                              [-0.01532488],
                              [-0.04731416],
                              [-0.01854842]])
        }

    updated_W_1 = np.array([[ 0.09725398,  0.19803019,  0.29917897,  0.40302941,  0.10131517],
                            [-0.09915837,  0.19624471, -0.30226154,  0.39931629,  0.20092314],
                            [-0.00644442, -0.0034085 ,  0.09559772,  0.20035532, -0.19879982]])

    updated_b_1 = np.array([[ 0.19929402],
                            [-0.20061234],
                            [ 0.09991286],
                            [-0.39998221],
                            [-0.19965396]])

    updated_W_2 = np.array([[ 3.00743977e-01, -1.98674963e-01,  2.21724200e-04, 1.97709262e-01],
                            [ 2.99047244e-01, -1.00886193e-01, -1.96635875e-01, 9.84748236e-02],
                            [ 4.00366019e-01,  2.04007424e-01,  1.00899928e-01, 1.94726629e-01],
                            [-1.02755626e-01, -2.48352760e-03,  8.87246900e-03, -2.03633316e-01],
                            [-9.93804688e-02, -1.48621800e-04, -1.26647000e-04, -2.00344262e-01]])

    updated_b_2 =  np.array([[ 0.39837625],
                             [-0.1996935 ],
                             [-0.39905372],
                             [ 0.50037097]])
    return conf, params, grad_params, updated_W_1, updated_b_1, updated_W_2, updated_b_2
