{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Train an image captioning network\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"*Image Captioning is the process of generating textual description of an image. It uses both Natural Language Processing and Computer Vision to generate the captions*\"\n",
    "\n",
    "In this mandaroty exercise you are implementing an image captioning network. The network will consist of an encoder and a decoder. The encoder is a convolutional neural network, and the decoder is a recurrent neural network. Producing reasonable textual description of an image is a hard task, however with the use of a CNN and a RNN we can start to generate somewhat plausible descriptions. \n",
    "\n",
    "\n",
    "Links:\n",
    "- [Task1: Implementation](#Task1)\n",
    "- [Task2: Train the recurrent neural network](#Task2)\n",
    "- [Task3: Generate image captions](#Task3)\n",
    "\n",
    "\n",
    "Software version:\n",
    "- Python 3.6\n",
    "- Pytorch 1.0\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Evaluation format ###\n",
    "\n",
    "\n",
    "You will be guided through the implementation step by step, and you can check your implementation at each step. Note, you will often need to complete all previous steps in order to continue.\n",
    "\n",
    "In the implementation part, test functions allow you to check your code rapidly. when your code works, you can apply it on part 2 to train and part 3 to generate captions. Part 2, the training process, is slow and can run over night on a typical laptop computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Exercise content\n",
    "\n",
    "\n",
    "All subtasks that you are to answer are found in this notebook. All implementation should be done in the file \"cocoSource.py\" found in folder \"/sourceFiles/\". The skeleton of the program is already implemented and contains things such as:\n",
    "- Importing data\n",
    "- Training framework\n",
    "- Saving and restoring models\n",
    "\n",
    "\n",
    "As mentioned, an image captioning network consists of an encoder and a decoder. Your task is to implement the decoder (RNN). The images have already been processed through a CNN. The feature vectors are stored in pickle files together with corresponding labels.\n",
    "\n",
    "\n",
    "During task 1, you will implement all required functionalities for training the image captioning network. In task 2, you will train the network and study how different RNN arcitectures influences the loss. You will generate image captions from images in the validation set in task 3. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Dataset ###\n",
    "\n",
    "We will use a dataset called \"Common Object in Context\" (COCO) 2017. It has ~120,000 training and 5,000 validation images. Every image also includes ~5 captions.\n",
    "\n",
    "![](utils_images/bear.png)\n",
    "\n",
    "Captions:\n",
    "\n",
    "- A big burly grizzly bear is show with grass in the background.\n",
    "- The large brown bear has a black nose.\n",
    "- Closeup of a brown bear sitting in a grassy area.\n",
    "- A large bear that is sitting on grass. \n",
    "- A close up picture of a brown bear's face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Network architecture ###\n",
    "\n",
    "**Encoder**\n",
    "\n",
    "\n",
    "Convolutional neural networks have shown to be useful for extracting high level features from images. We will use a pretrained VGG16 network trained on ImageNet. ImageNet consists of 1,2 million images distributed over 1000 classes, and we hope the model have learnt many general features. We are not interested in classifying the 1000 classes, and will change the last fully connected layer with our own. We will use a tanh actiation function to squeeze the values between -1 and 1 similar to the recurrent cells. \n",
    "\n",
    "**Decoder**\n",
    "\n",
    "To be able to convert the high level features from the encoder to natural language, we will construct a recurrent neural network. The output of the encoder will be passed as the initial state to the recurrent cells. The input to the recurret neural network will be word embeddings which we will learn. \n",
    "\n",
    "**Loss function**\n",
    "\n",
    "The words will be considered as separate classes and we shall use cross entropy loss.\n",
    "\n",
    "**Training vs testing**\n",
    "\n",
    "When we train the RNN, we will feed in the correct token (word) for every time step, see figure 2a. The words (tokens) are generally unknown, and in test mode, we will need to use our best estimate as input. Example, if the word \"brown\" has the highest probabillity after the softmax at timestep 2, we will feed in \"brown\" as input at timestep 3, see figure 2b.\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/image_captioning_diagram_train.png\", width=2200>\n",
    "Figure 2a: The figure shows an example with 3 recurrent cells stacked in train mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/image_captioning_diagram_test.png\", width=2200>\n",
    "Figure 2b: The figure shows an example with 3 recurrent cells stacked in test mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Vocabulary ###\n",
    "\n",
    "\n",
    "To make good decisions in terms of the network architecture, we will need to know the statistics of the captions:\n",
    "- The number of words in the captions (sequence length) should be considered when chooseing the truncated backpropagation length. \n",
    "- To save memory, it is normal to limit the vocabulary size/length. There will be a tradeoff between capturing all words and have a reansonble sized softmax layer. \n",
    "\n",
    "\n",
    "Note: The captions have been filtered such that all special characters have been removed. This includes also punctuations and commas. All characters have been changed to lower case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "**Vocabulary size**\n",
    "\n",
    "To make good predictions we need the model to have the abillity to predict frequent words. Figure 3 shows a sorted histogram of the word count for the different words. We can see that the majority of the words are within the 2000 most frequent words. Figure 4 shows the precentage of the words accounted for as a function of the vocabulary size.\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/Word_count_hist.png\", width=800>\n",
    "*Figure 3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/accumulated_Word_count.png\",width=800>\n",
    "*Figure 4*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Truncated backpropagation length**\n",
    "\n",
    "\n",
    "The sequence length of the recurrent neural network should be able to capture the time(step) dependencies within the captions. Figure 5 shows a histogram of the caption counts as a function of the caption (/sequence) length. Figure 6 shows the precentage of all captions with shorter or equal caption length as a function of caption (/sequence) length. \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/Sequence_lengths_hist.png\", width=800>\n",
    "*Figure 5*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/accumulated_Sequence_length_count.png\", width=800>\n",
    "*Figure 6*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Common words and their tokens**\n",
    "\n",
    "\n",
    "Every word is assosiated with a token. The words are sorted such that the most frequent words have lower token values. In the table below, you find the six most common words in the dataset. The three first words/tokens have a spescial purpuse:\n",
    "- \"eeee\": Indicates the end of a caption.\n",
    "- \"ssss\": Start token, first word \n",
    "- \"UNK\": As the vocabulary size often is smaller than the number of words in the datasset, we change all \"unknown\" words to the word \"UNK\"\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"utils_images/vocabulary.png\", width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "### Word embedding  ###\n",
    "\n",
    "\n",
    "The input to the first rnn layer is the embedded version of the input words. We will define a word embedding matrix with shape [vocabulary_size, embedding_size)]. A single word embedding will be a row vector with length [embedding_size]. The tokens are used to select the correct row within the word embedding matrix. For example, the word \"a\" will have values in the third row in the embedding matrix (where first row is row zero).\n",
    "\n",
    "\n",
    "a_emb = wordEmbedding[a_token, :]\n",
    "\n",
    "\n",
    "The word embedding matrix will be initialized with random values and they will be updated as part of the training process. The goal is that similar words get similar vector representations within the embedding matrix. This will not be part of the assigment, but as a test, the vector representations could be embedded using e.g. t-SNE for plotting in 2D/3D space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "### Data preparation  ###\n",
    "\n",
    "Before starting to work on implementing the network aritecture, you should check if you need to download and process data:\n",
    "\n",
    "- **UIO**: If you work on a IFI computer or one of the ML servers, we have already processed and made the data avaiable for you. The paths are given in the notebook.\n",
    "\n",
    "- **Personal computer**: If you plan to work on the assigment on your personal computer, you will need to download and process the data yourself. Follow the steps in notebook \"data_preparation.ipynb\". Running the data preparation can take several hours. \n",
    "\n",
    "*Change the configuration below if you use CPU or GPU*\n",
    "\n",
    "\n",
    "\n",
    "The processed data are pickle files holding the following information for every image:\n",
    "- Path to the jpg image\n",
    "- Captions\n",
    "- Captions given as tokens\n",
    "- The fc7 output from the VGG16 network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task1'></a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Task 1: Implementation #\n",
    "See function headings for specification of each function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** General advice**\n",
    "\n",
    "Do not regenerate the data if you don't need it, it will run overnight on a laptop without GPU. Use either /opt/ifi/anaconda3/bin/jupyter notebook or the ml-servers. \n",
    "\n",
    "Part 1 is for developing your code and runs fairly fast. \n",
    "Part 2 does the actual captioning and can take some hours on CPU. \n",
    "\n",
    "\n",
    "Restarting the notebook after each subtask can be a good idea. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1a** \n",
    "\n",
    "Your are to implement a vanailla RNN class in the function RNNCell in file sourceFiles/cocoSource.py. The class shall have a constructor and the function \"forward\". To speed up inference, we will concatinate the input and the old state matrices and perform a single matrix multiplication. The difference is illustrated below.\n",
    "\n",
    "Vanilla RNN:\n",
    "\n",
    "\\begin{align}\n",
    "h_t &= tanh(x_tW^{hx} + h_{t-1}W^{hh} + b) \\\\\n",
    "\\\\\n",
    "h_t &= tanh([x_t,h_{t-1}]W + b)\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "import torch\n",
    "\n",
    "#Defining dummy variables\n",
    "hidden_state_sizes = 512\n",
    "inputSize  = 256\n",
    "batch_size = 128\n",
    "x          = torch.tensor(torch.rand(batch_size, inputSize))\n",
    "state_old  = torch.tensor(torch.rand(batch_size, hidden_state_sizes))\n",
    "\n",
    "# You should implement this function\n",
    "cell   = cocoSource.RNNCell(hidden_state_sizes, inputSize)\n",
    "cell(x, state_old)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.RNNcell_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1b** \n",
    "\n",
    "Your are to implement a Gated recurrent units (GRU) class in the function GRUCell in cocoSource.py. The class shall have a constructor and the function \"forward\". To speed up inference, we will concatinate the input and the old state matrices and perform a single matrix multiplication. The difference is illustrated in the equations below, where you are to implement the equations in the column to the right.\n",
    "\n",
    "GRU:\n",
    "\n",
    "\\begin{align}\n",
    "&Update \\: gate: \\qquad &\\Gamma^u=\\sigma(x_tW^{u} + h_{t-1}U^{u} + b^u) \\qquad \\rightarrow \\qquad &\\Gamma^u=\\sigma([x_t, h_{t-1}]W^{u} + b^u) \\\\\n",
    "&Reset \\: gate: \\qquad &\\Gamma^r=\\sigma(x_tW^{r} + h_{t-1}U^{r} + b^r) \\qquad \\rightarrow \\qquad &\\Gamma^r=\\sigma([x_t, h_{t-1}]W^{r} + b^r) \\\\\n",
    "&Candidate \\:cell: &\\tilde{h_t} = tanh([x_tW + (\\Gamma^r \\circ h_{t-1})U + b) \\qquad \\rightarrow \\qquad &\\tilde{h_t} = tanh([x_t, (\\Gamma^r \\circ h_{t-1})] W + b) \\\\\n",
    "&Final\\: cell:    &h_t = \\Gamma^u \\circ h_{t-1} + (1-\\Gamma^u) \\circ \\tilde{h_t} \\qquad \\rightarrow \\qquad &h_t = \\Gamma^u \\circ h_{t-1} + (1-\\Gamma^u) \\circ \\tilde{h_t}\n",
    "\\end{align}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "import torch\n",
    "\n",
    "#Defining dummy variables\n",
    "hidden_state_sizes = 512\n",
    "inputSize  = 648\n",
    "batch_size = 128\n",
    "x          = torch.tensor(torch.rand(batch_size, inputSize))\n",
    "state_old  = torch.tensor(torch.rand(batch_size, hidden_state_sizes))\n",
    "\n",
    "# You should implement this function\n",
    "cell   = cocoSource.GRUCell(hidden_state_sizes, inputSize)\n",
    "cell(x, state_old)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.GRUcell_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1c** \n",
    "\n",
    "Your task is to implement \"loss_fn\" in cocoSource.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "import torch\n",
    "\n",
    "weight_dir = 'unit_tests/loss_fn_tensors.pt'\n",
    "checkpoint = torch.load(weight_dir)\n",
    "    \n",
    "logits     = checkpoint['logits']\n",
    "yTokens    = checkpoint['yTokens']\n",
    "yWeights   = checkpoint['yWeights']\n",
    "\n",
    "sumLoss, meanLoss = cocoSource.loss_fn(logits, yTokens, yWeights)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.loss_fn_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1d** \n",
    "\n",
    "You may want to have a look at \"utils/trainer.py\" and \"utils/model.py\" to understand the flow of the script.\n",
    "Your task is to implement the  RNN class in cocoSource.py. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Config\n",
    "my_dir = 'unit_tests/RNN_tensors_is_train_True.pt'\n",
    "checkpoint = torch.load(my_dir)\n",
    "\n",
    "outputLayer   = checkpoint['outputLayer']\n",
    "Embedding     = checkpoint['Embedding']\n",
    "xTokens       = checkpoint['xTokens']\n",
    "initial_hidden_state  = checkpoint['initial_hidden_state']\n",
    "input_size            = checkpoint['input_size']\n",
    "hidden_state_size     = checkpoint['hidden_state_size']\n",
    "num_rnn_layers        = checkpoint['num_rnn_layers']\n",
    "cell_type             = checkpoint['cell_type']\n",
    "is_train              = checkpoint['is_train']\n",
    "\n",
    "# You should implement this function\n",
    "myRNN = cocoSource.RNN(input_size, hidden_state_size, num_rnn_layers, cell_type)\n",
    "logits, current_state = myRNN(xTokens, initial_hidden_state, outputLayer, Embedding, is_train)\n",
    "\n",
    "#Check implementation\n",
    "is_train = True\n",
    "unit_tests.RNN_test(is_train)\n",
    "\n",
    "is_train = False\n",
    "unit_tests.RNN_test(is_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Task 1e** \n",
    "\n",
    "You should now implement the imageCaptionModel class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "from sourceFiles import cocoSource\n",
    "from unit_tests import unit_tests\n",
    "\n",
    "#Config\n",
    "my_dir = 'unit_tests/imageCaptionModel_tensors.pt'\n",
    "checkpoint = torch.load(my_dir)\n",
    "config                            = checkpoint['config']\n",
    "vgg_fc7_features                  = checkpoint['vgg_fc7_features']\n",
    "xTokens                           = checkpoint['xTokens']\n",
    "is_train                          = checkpoint['is_train']\n",
    "myImageCaptionModelRef_state_dict = checkpoint['myImageCaptionModelRef_state_dict']\n",
    "logitsRef                         = checkpoint['logitsRef']\n",
    "current_hidden_state_Ref          = checkpoint['current_hidden_state_Ref']\n",
    "\n",
    "myImageCaptionModel = cocoSource.imageCaptionModel(config)\n",
    "logits, current_hidden_state = myImageCaptionModel(vgg_fc7_features, xTokens,  is_train)\n",
    "\n",
    "#Check implementation\n",
    "unit_tests.imageCaptionModel_test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task2'></a>\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "# Task 2: Train the image captioning network #\n",
    "\n",
    "\n",
    "Congratulations, you are done with all implemention. You shall now train various network architectures. The weights will be stored regularly, but updated when the validation loss has decrease only. When you are done training you can go to task 3 to start generating captions. \n",
    "\n",
    "Depending on your compute power, the training can take a long time. Start with small simple network architectures.\n",
    "\n",
    "The loss per epoch images will be stored in folder \"loss_images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataLoader import DataLoaderWrapper\n",
    "from utils.saverRestorer import SaverRestorer\n",
    "from utils.model import Model\n",
    "from utils.trainer import Trainer\n",
    "from utils.validate import plotImagesAndCaptions\n",
    "\n",
    "#Path if you work on personal computer\n",
    "data_dir = 'data/coco/'\n",
    "\n",
    "#Path if you work on UIO IFI computer\n",
    "#data_dir = '/projects/in5400/oblig2/coco/'\n",
    "\n",
    "#Path if you work on one of the ML servers\n",
    "#data_dir = '/shared/in5400/coco/'\n",
    "\n",
    "#train\n",
    "modelParam = {\n",
    "        'batch_size': 128,          # Training batch size\n",
    "        'cuda': {'use_cuda': False,  # Use_cuda=True: use GPU\n",
    "                 'device_idx': 0},  # Select gpu index: 0,1,2,3\n",
    "        'numbOfCPUThreadsUsed': 3,  # Number of cpu threads use in the dataloader\n",
    "        'numbOfEpochs': 20,         # Number of epochs\n",
    "        'data_dir': data_dir,       # data directory\n",
    "        'img_dir': 'loss_images/',\n",
    "        'modelsDir': 'storedModels/',\n",
    "        'modelName': 'model_0/',    # name of your trained model\n",
    "        'restoreModelLast': 0,\n",
    "        'restoreModelBest': 0,\n",
    "        'modeSetups':   [['train', True], ['val', True]],\n",
    "        'inNotebook': True,         # If running script in jupyter notebook\n",
    "        'inference': False\n",
    "}\n",
    "\n",
    "config = {\n",
    "        'optimizer': 'adam',             # 'SGD' | 'adam' | 'RMSprop'\n",
    "        'learningRate': {'lr': 0.0005},  # learning rate to the optimizer\n",
    "        'weight_decay': 0,               # weight_decay value\n",
    "        'VggFc7Size': 4096,              # Fixed, do not change\n",
    "        'embedding_size': 128,           # word embedding size\n",
    "        'vocabulary_size': 4000,        # number of different words\n",
    "        'truncated_backprop_length': 20,\n",
    "        'hidden_state_sizes': 256,       #\n",
    "        'num_rnn_layers': 2,             # number of stacked rnn's\n",
    "        'cellType': 'RNN'                # RNN or GRU\n",
    "        }\n",
    "\n",
    "\n",
    "# create an instance of the model you want\n",
    "model = Model(config, modelParam)\n",
    "\n",
    "# create an instacne of the saver and resoterer class\n",
    "saveRestorer = SaverRestorer(config, modelParam)\n",
    "model        = saveRestorer.restore(model)\n",
    "\n",
    "# create your data generator\n",
    "dataLoader = DataLoaderWrapper(config, modelParam)\n",
    "\n",
    "# here you train your model\n",
    "trainer = Trainer(model, modelParam, config, dataLoader, saveRestorer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Task3'></a>\n",
    "\n",
    "---\n",
    "\n",
    "# Task 3: Generate image captions on the validation set#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "# This is a bit of magic to make matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataLoader import DataLoaderWrapper\n",
    "from utils.saverRestorer import SaverRestorer\n",
    "from utils.model import Model\n",
    "from utils.trainer import Trainer\n",
    "from utils.validate import plotImagesAndCaptions\n",
    "\n",
    "#Path if you work on personal computer\n",
    "data_dir = 'data/coco/'\n",
    "\n",
    "#Path if you work on UIO IFI computer\n",
    "#data_dir = /projects/in5400/oblig2/coco/\n",
    "\n",
    "#Path if you work on one of the ML servers\n",
    "#data_dir = '/shared/in5400/coco/'\n",
    "\n",
    "modelParam = {\n",
    "        'batch_size': 128,          # Training batch size\n",
    "        'cuda': {'use_cuda': False,  # Use_cuda=True: use GPU\n",
    "                 'device_idx': 0},  # Select gpu index: 0,1,2,3\n",
    "        'numbOfCPUThreadsUsed': 3,  # Number of cpu threads use in the dataloader\n",
    "        'numbOfEpochs': 20,         # Number of epochs\n",
    "        'data_dir': data_dir,       # data directory\n",
    "        'img_dir': 'loss_images/',\n",
    "        'modelsDir': 'storedModels/',\n",
    "        'modelName': 'model_0/',    # name of your trained model\n",
    "        'restoreModelLast': 0,\n",
    "        'restoreModelBest': 0,\n",
    "        'modeSetups':   [['train', True], ['val', True]],\n",
    "        'inNotebook': True,         # If running script in jupyter notebook\n",
    "        'inference': True\n",
    "}\n",
    "\n",
    "config = {\n",
    "        'optimizer': 'adam',             # 'SGD' | 'adam' | 'RMSprop'\n",
    "        'learningRate': {'lr': 0.0005},  # learning rate to the optimizer\n",
    "        'weight_decay': 0,               # weight_decay value\n",
    "        'VggFc7Size': 4096,              # Fixed, do not change\n",
    "        'embedding_size': 128,           # word embedding size\n",
    "        'vocabulary_size': 4000,        # number of different words\n",
    "        'truncated_backprop_length': 20,\n",
    "        'hidden_state_sizes': 256,       #\n",
    "        'num_rnn_layers': 2,             # number of stacked rnn's\n",
    "        'cellType': 'RNN'                # RNN or GRU\n",
    "        }\n",
    "\n",
    "if modelParam['inference'] == True:\n",
    "    modelParam['batch_size'] = 1\n",
    "    modelParam['modeSetups'] = [['val', False]]\n",
    "    modelParam['restoreModelBest'] = 1\n",
    "\n",
    "# create an instance of the model you want\n",
    "model = Model(config, modelParam)\n",
    "\n",
    "# create an instacne of the saver and resoterer class\n",
    "saveRestorer = SaverRestorer(config, modelParam)\n",
    "model        = saveRestorer.restore(model)\n",
    "\n",
    "# create your data generator\n",
    "dataLoader = DataLoaderWrapper(config, modelParam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the cell below to generate captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotImagesAndCaptions\n",
    "plotImagesAndCaptions(model, modelParam, config, dataLoader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
