{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='GeForce RTX 2070', major=7, minor=5, total_memory=7951MB, multi_processor_count=36)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# It is best to set device=\"cuda:0\" each time a tensor is created\n",
    "# instead of .to(device) with device as specified  \n",
    "# accordingly:\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# Then, the data is created directly on the CUDA GPU \n",
    "# with no CPU (RAM) memory handling first.\n",
    "\n",
    "# Convert from tensor on GPU to to numpy array on CPU.\n",
    "#x_cpu = x.cpu().numpy()\n",
    "\n",
    "# Check which device a tensor is on\n",
    "#x.device\n",
    "\n",
    "# Check if CUDA is available\n",
    "#torch.cuda.is_available()\n",
    "\n",
    "# CUDA device properties\n",
    "torch.cuda.get_device_properties(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edited ~/.spimagine\n",
    "# colormap = grays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui qt5\n",
    "import spimagine\n",
    "from spimagine import volshow, volfig\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from glob import glob\n",
    "from pydicom.filereader import dcmread\n",
    "\n",
    "#\"\"\"\n",
    "gaussian_kernel_width = 40\n",
    "gaussian_std = 10\n",
    "\n",
    "# Loading MRI volume to convolve\n",
    "aquisition_image_path = \"/media/ivar/HDD3TB2/IN9400_exercises/Ivar_MRI/FLAIR 3D/DICOM\"\n",
    "image_slices_unsorted = glob(aquisition_image_path + \"/IM*\")\n",
    "def get_image_number(file):\n",
    "    return int(file[len(file)-list(reversed(file)).index(\"_\"):])\n",
    "image_slices = sorted(image_slices_unsorted, key=get_image_number)\n",
    "\n",
    "# Swapping and flipping axes to get it right\n",
    "x_np = np.array([dcmread(s).pixel_array for s in image_slices if np.str(dcmread(s).SeriesNumber) == \"1301\"])\n",
    "x_np = np.swapaxes(x_np, 0, 2)\n",
    "x_np = np.swapaxes(x_np, 0, 1)\n",
    "x_np = np.flip(x_np, 0)\n",
    "x_np = np.flip(x_np, 1)\n",
    "\n",
    "# Copying the data to GPU\n",
    "x = torch.as_tensor(np.int32(x_np), dtype=torch.double, device=device)\n",
    "# Important: Mean center and scale by std for x\n",
    "x -= torch.mean(x)\n",
    "x /= torch.std(x)\n",
    "\n",
    "# Convolution kernel/weights .\n",
    "# A broadcasted 2D gaussian kernel to accomplish smoothing\n",
    "gaussian = signal.gaussian(gaussian_kernel_width, gaussian_std).reshape((gaussian_kernel_width, 1))\n",
    "w_np = gaussian*(gaussian.T)*gaussian.reshape((gaussian_kernel_width, 1, 1))\n",
    "w = torch.as_tensor(w_np, dtype=torch.double, device=device)\n",
    "# Make shure sum of w equals 1\n",
    "w -= torch.mean(w)\n",
    "w /= torch.std(w)\n",
    "\n",
    "# Bias term\n",
    "b = torch.randn(1, device=device)\n",
    "#\"\"\"\n",
    "\n",
    "def show_volume_spimagine(volume_data_pytorch_gpu_tensor, stackUnits=(1, 1, 1)):\n",
    "    volfig()\n",
    "    spim_widget = \\\n",
    "    volshow(volume_data_pytorch_gpu_tensor.cpu().numpy(), stackUnits=stackUnits, interpolation='nearest')\n",
    "\n",
    "def show_volume_spimagine_cpu(numpy_array, stackUnits=(1, 1, 1)):\n",
    "    volfig()\n",
    "    spim_widget = \\\n",
    "    volshow(numpy_array, stackUnits=stackUnits, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4319c53828>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizing a slice of the gaussian kernel\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.imshow(w[gaussian_kernel_width//2,:,:].cpu().numpy(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512, 365])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement 3D convolution.\n",
    "import torch.nn as nn\n",
    "\n",
    "def compute_convolved_image_shape_3D(input_depth, \\\n",
    "                                     input_height, \\\n",
    "                                     input_width, \\\n",
    "                                     kernel_depth, \\\n",
    "                                     kernel_height, \\\n",
    "                                     kernel_width, \\\n",
    "                                     pad_size, \\\n",
    "                                     stride):\n",
    "    # Formula for the spatial dimensions of an image\n",
    "    # in an convolutional output layer.\n",
    "    return torch.as_tensor(np.int(np.floor(1 + (input_depth + 2*pad_size - kernel_depth)/stride)), device=device), \\\n",
    "            torch.as_tensor(np.int(np.floor(1 + (input_height + 2*pad_size - kernel_height)/stride)), device=device), \\\n",
    "            torch.as_tensor(np.int(np.floor(1 + (input_width + 2*pad_size - kernel_width)/stride)), device=device)\n",
    "\n",
    "def convolution_3D(input_image, weights, pad_size=1, stride=1, save_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    Write a general function to convolve an image with an arbitrary kernel.\n",
    "    \"\"\"\n",
    "    # Flipping weights/kernel to so that it is convolution and not correlation.\n",
    "    # Did not work with pytorch tensor\n",
    "    #weights = weights[::-1, ::-1, ::-1]\n",
    "\n",
    "    # He scaling of weights .\n",
    "    #weights = torch.div(weights, torch.sqrt(torch.tensor(2/weights.numel())))\n",
    "    \n",
    "    \n",
    "    depth_weights, height_weights, width_weights = weights.shape\n",
    "    \n",
    "    depth_input_image, height_input_image, width_input_image = input_image.shape\n",
    "    \n",
    "    padding = nn.ConstantPad3d(pad_size, 0)\n",
    "    \n",
    "    input_image_padded = padding(input_image)\n",
    "    #np.pad(input_image, ((pad_size,), (pad_size,), (pad_size,)), mode=\"constant\", constant_values=0)\n",
    "    \n",
    "    # Define dimension of output image.\n",
    "    depth_output_image, height_output_image, width_output_image,  = \\\n",
    "    compute_convolved_image_shape_3D(depth_input_image, \\\n",
    "                                     height_input_image, \\\n",
    "                                     width_input_image, \\\n",
    "                                     depth_weights, \\\n",
    "                                     height_weights, \\\n",
    "                                     width_weights, \\\n",
    "                                     pad_size, \\\n",
    "                                     stride)\n",
    "    \n",
    "    output_image = torch.zeros((depth_output_image, \\\n",
    "                                height_output_image, \\\n",
    "                                width_output_image), \\\n",
    "                               device=device)\n",
    "    \n",
    "    # Zero padding output image to have equal dimensions\n",
    "    # as temporary_volume_to_save .\n",
    "    # Assuming equal padding in all directions.\n",
    "    #in_out_image_axis_offset = ((input_image_padded.shape[0] - output_image.shape[0])//2)//stride\n",
    "    \n",
    "    accumulated_convolved_image = torch.zeros_like(input_image_padded)\n",
    "    deaccumulated_and_accumulated_convolved_input_image = input_image_padded.clone()\n",
    "    \n",
    "    save_number = 1\n",
    "    iteration = 0\n",
    "    save_interval = 10\n",
    "    \n",
    "    # Firt perform the entire convolution\n",
    "    for z in range(depth_output_image):\n",
    "        for y in range(height_output_image):\n",
    "            for x in range(width_output_image):\n",
    "                                \n",
    "                    # The convolution\n",
    "                    input_image_padded_masked = \\\n",
    "                    input_image_padded[z*stride:z*stride+depth_weights, y*stride:y*stride+height_weights, x*stride:x*stride+width_weights]\n",
    "\n",
    "                    product = input_image_padded_masked * weights\n",
    "                    \n",
    "                    output_image[z, y, x] = \\\n",
    "                    torch.sum(product)\n",
    "                    \n",
    "    # Mean normalize the output\n",
    "    output_image -= torch.mean(output_image)\n",
    "    output_image /= torch.std(output_image)\n",
    "    \n",
    "    # Do the entire convolution again to create the animated sequence with normalized output convolution.\n",
    "    for z in range(depth_output_image):\n",
    "        for y in range(height_output_image):\n",
    "            for x in range(width_output_image):\n",
    "                \n",
    "                    iteration += 1\n",
    "                \n",
    "                    #if iteration == 20:\n",
    "                    #    return output_image\n",
    "                    \n",
    "                    # Redo the element wise multiplication of the convolution.\n",
    "                    input_image_padded_masked = \\\n",
    "                    input_image_padded[z*stride:z*stride+depth_weights, y*stride:y*stride+height_weights, x*stride:x*stride+width_weights]\n",
    "                    \n",
    "                    product = input_image_padded_masked * weights\n",
    "                    \n",
    "                    # Normalize the product according to entire output image.\n",
    "                    product -= torch.mean(output_image)\n",
    "                    product /= torch.std(output_image)\n",
    "                    \n",
    "                    # THIS WAS NOT NECESSARY, start\n",
    "                    # Save snapshot of what is left of input image to convolute.\n",
    "                    #torch.save(deaccumulated_and_accumulated_convolved_input_image, \"data/\"+str(save_number)+\".pt\")\n",
    "                    \n",
    "                    #save_number += 1\n",
    "                    # THIS WAS NOT NECESSARY, end\n",
    "                    \n",
    "                    # Get the stored convoluted image so far.\n",
    "                    temporary_volume_to_save \\\n",
    "                    = deaccumulated_and_accumulated_convolved_input_image.clone()\n",
    "                    \n",
    "                    # update temporary_volume_to_save with the element.wise multiplicated area.\n",
    "                    temporary_volume_to_save[z*stride:z*stride+depth_weights, y*stride:y*stride+height_weights, x*stride:x*stride+width_weights]\\\n",
    "                    = product\n",
    "                    \n",
    "                    #if iteration % save_interval == 0:\n",
    "                    # Save temporary_volume_to_save for\n",
    "                    # 10 intermediate steps for later visualization.\n",
    "                    if (iteration >= (depth_output_image*height_output_image*width_output_image)//2 + 200 and \\\n",
    "                        iteration <= (depth_output_image*height_output_image*width_output_image)//2 + 210):\n",
    "                        #\"\"\"\n",
    "                        # Save this view (pre-convolution step, shows the product).\n",
    "                        # And previous convoluted parts of image\n",
    "                        torch.save(temporary_volume_to_save, save_dir+\"/\"+str(save_number)+\".pt\")\n",
    "                        save_number += 1\n",
    "                        #\"\"\"\n",
    "                    \n",
    "                    # Store the actual convolution output in accumulated_convolved_image .\n",
    "                    # Using normalized output pixel from previous convolition and\n",
    "                    # noramlization step (mean centering and divide by std).\n",
    "                    accumulated_convolved_image[z*stride+depth_weights//2, \\\n",
    "                                                y*stride+height_weights//2, \\\n",
    "                                                x*stride+width_weights//2]\\\n",
    "                    = output_image[z, y, x].clone()\n",
    "                    \n",
    "                    # Update temporary_volume_to_save and overwrite\n",
    "                    # deaccumulated_and_accumulated_convolved_input_image\n",
    "                    # with a copy of it.\n",
    "                    temporary_volume_to_save[0:z*stride+depth_weights, \\\n",
    "                                             0:y*stride+height_weights, \\\n",
    "                                             0:x*stride+width_weights] \\\n",
    "                    = accumulated_convolved_image[0:z*stride+depth_weights, \\\n",
    "                                                  0:y*stride+height_weights, \\\n",
    "                                                  0:x*stride+width_weights]\n",
    "                    \n",
    "                    deaccumulated_and_accumulated_convolved_input_image \\\n",
    "                    = temporary_volume_to_save.clone()\n",
    "    \n",
    "    # Save copies of the convolved image with input image shape.\n",
    "    num_copies_to_save = 1\n",
    "    for i in range(num_copies_to_save):\n",
    "        torch.save(accumulated_convolved_image, save_dir+\"/\"+str(save_number)+\".pt\")\n",
    "        save_number += 1\n",
    "    \n",
    "    if stride > 1:\n",
    "        # Render images for animated high-res -> low-res\n",
    "        # affine scaling.\n",
    "        \n",
    "        print(output_image.shape)\n",
    "        accumulated_convolved_image_upscaled = \\\n",
    "        torch.zeros_like(output_image, device=device)\n",
    "        \n",
    "        # Get the relevant pixels from accumulated_convolved_image .\n",
    "        for z in range(depth_output_image):\n",
    "            for y in range(height_output_image):\n",
    "                for x in range(width_output_image):\n",
    "                    accumulated_convolved_image_upscaled[z, \\\n",
    "                                                         y, \\\n",
    "                                                         x] \\\n",
    "                    = accumulated_convolved_image[z*stride+depth_weights//2, \\\n",
    "                                                  y*stride+height_weights//2, \\\n",
    "                                                  x*stride+width_weights//2]\n",
    "        \n",
    "        # Interpolate accumulated_convolved_image from having \n",
    "        # shape like output_image to shape like accumulated_convolved_image .\n",
    "        \n",
    "        # Needed to reshape into (mini-batch, channels, depth, heigth, width)\n",
    "        # for intepolate to understand the volume.\n",
    "        # Also need to compute the scale factor, based on the formula used in\n",
    "        # compute_convolved_image_shape_3D .\n",
    "        #\"\"\"\n",
    "        scale_factor_z = stride*(2*pad_size + depth_input_image)/\\\n",
    "        (stride + depth_input_image + 2*pad_size - depth_weights)\n",
    "\n",
    "        scale_factor_y = stride*(2*pad_size + height_input_image)/\\\n",
    "        (stride + height_input_image + 2*pad_size - height_weights)\n",
    "\n",
    "        scale_factor_x = stride*(2*pad_size + width_input_image)/\\\n",
    "        (stride + width_input_image + 2*pad_size - width_weights)\n",
    "        #\"\"\"\n",
    "        \n",
    "        accumulated_convolved_image_upscaled = \\\n",
    "        nn.functional.interpolate(torch.reshape(accumulated_convolved_image_upscaled, (1, \\\n",
    "                                                                                      1, \\\n",
    "                                                                                      accumulated_convolved_image_upscaled.shape[0], \\\n",
    "                                                                                       accumulated_convolved_image_upscaled.shape[1], \\\n",
    "                                                                                       accumulated_convolved_image_upscaled.shape[2]\n",
    "                                                                                      )), \\\n",
    "                                  scale_factor=(scale_factor_z, \\\n",
    "                                                scale_factor_y, \\\n",
    "                                                scale_factor_x), \\\n",
    "                                  mode=\"nearest\")\n",
    "        \"\"\"\n",
    "        # Replication pad the rest of the missing shape size.\n",
    "        pad_z = \\\n",
    "        (input_image_padded.shape[0] - accumulated_convolved_image_upscaled.shape[2]) // 2\n",
    "        pad_y = \\\n",
    "        (input_image_padded.shape[1] - accumulated_convolved_image_upscaled.shape[3]) // 2\n",
    "        pad_x = \\\n",
    "        (input_image_padded.shape[2] - accumulated_convolved_image_upscaled.shape[4]) // 2\n",
    "        \n",
    "        padding = nn.ReplicationPad3d((pad_x, pad_x, pad_y, pad_y, pad_z, pad_z))\n",
    "        accumulated_convolved_image_upscaled = \\\n",
    "        padding(accumulated_convolved_image_upscaled)\n",
    "        \"\"\"\n",
    "        \n",
    "        #\"\"\"\n",
    "        # Reshape back to (depth, heigth, width) before saving.\n",
    "        accumulated_convolved_image_upscaled_reshaped = \\\n",
    "        torch.reshape(accumulated_convolved_image_upscaled, \\\n",
    "                      (accumulated_convolved_image_upscaled.shape[2], \\\n",
    "                       accumulated_convolved_image_upscaled.shape[3], \\\n",
    "                       accumulated_convolved_image_upscaled.shape[4]))\n",
    "        \n",
    "        # Trick to make shure that the interpolated image has exactly\n",
    "        # equal dimensions to padded input image\n",
    "        # using zero padding.\n",
    "        accumulated_convolved_image_upscaled_reshaped_padded = \\\n",
    "        torch.zeros_like(input_image_padded)\n",
    "        accumulated_convolved_image_upscaled_reshaped_padded[:accumulated_convolved_image_upscaled_reshaped.shape[0], \\\n",
    "                                                             :accumulated_convolved_image_upscaled_reshaped.shape[1], \\\n",
    "                                                             :accumulated_convolved_image_upscaled_reshaped.shape[2]] \\\n",
    "        = accumulated_convolved_image_upscaled_reshaped\n",
    "        \"\"\"\n",
    "        Not necessary to normalize \n",
    "        accumulated_convolved_image_upscaled_reshaped \n",
    "        since it is already normalized through the \n",
    "        commands above:\n",
    "        \n",
    "        # Mean normalize the output\n",
    "        output_image -= torch.mean(output_image)\n",
    "        output_image /= torch.std(output_image)\n",
    "        \"\"\"\n",
    "         \n",
    "        # Save copies of this image.\n",
    "        for i in range(num_copies_to_save):\n",
    "        \n",
    "            torch.save(accumulated_convolved_image_upscaled_reshaped, \\\n",
    "                       save_dir+\"/\"+str(save_number)+\".pt\")\n",
    "            save_number += 1\n",
    "        \n",
    "    # Lastly, save the actual output image.\n",
    "    torch.save(output_image, save_dir+\"/\"+str(save_number)+\".pt\")\n",
    "    save_number += 1\n",
    "        \n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the convolution\n",
    "run_output_folder = \"mri_all_normalized_nostride\"\n",
    "save_dir = \"data/\" + run_output_folder\n",
    "\n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# From the mri run\n",
    "#z = convolution_3D(x, w, pad_size=30, stride=50, save_dir=save_dir)# + b\n",
    "\n",
    "# From the mri_all_normalized run\n",
    "#z = convolution_3D(x, w, pad_size=30, stride=25, save_dir=save_dir)# + b\n",
    "\n",
    "# From the mri_all_normalized_nostride run\n",
    "z = convolution_3D(x, w, pad_size=30, stride=1, save_dir=save_dir)# + b\n",
    "\n",
    "#print(x.shape)\n",
    "#print(w.shape)\n",
    "#print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_volume_spimagine(x)\n",
    "#show_volume_spimagine(w)\n",
    "#show_volume_spimagine(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "\n",
    "#x_fft = torch.rfft(x, signal_ndim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Load stored timeseries data of the convolution.\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "run_output_folder = \"mri_all_normalized_nostride\"\n",
    "save_dir = \"data/\" + run_output_folder\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "aquisition_image_path = \"data/\" + run_output_folder\n",
    "images_unsorted = glob(aquisition_image_path + \"/*.pt*\")\n",
    "def get_image_number(file):\n",
    "    return int(file[len(save_dir + \"/\"):-len(\".pt\")])\n",
    "images = sorted(images_unsorted, key=get_image_number)\n",
    "\n",
    "#selected_image_index = len(images) // 2\n",
    "#image_selection = [images[0]] + [images[len(images)//2]] + [images[-2]] + [images[-1]]\n",
    "#image_selection = [images[-(501 + 501)]] + [images[-501]]\n",
    "#image_selection = [images[selected_image_index + i] for i in range(3)]\n",
    "#image_selection = [images[-3]] + [images[-2]]\n",
    "image_selection = [images[-4]]\n",
    "\n",
    "#z_series = np.array([torch.load(image).cpu().numpy() for image in image_selection])\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[z.shape for z in z_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show_volume_spimagine_cpu(z_series, stackUnits=(0.5, 0.48828125, 0.48828125))\n",
    "# render pngs with spimagine.\n",
    "# create gif with\n",
    "# ffmpeg -i output_%03d.png -filter:v \"setpts=5.0*PTS\" rendering_5x_slow_vol_35_gauss_w_22_std_12_all_normalized.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images[:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Convert torch pt no numpy npy format\n",
    "numpy_save_dir = save_dir + \"/\" + \"npy\"\n",
    "\n",
    "import os\n",
    "if not os.path.exists(numpy_save_dir):\n",
    "    os.makedirs(numpy_save_dir)\n",
    "    \n",
    "for i, image in enumerate(images[:-4]):\n",
    "    np.save(numpy_save_dir + \"/\" + str(i+1), torch.load(image).cpu().numpy())\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spimenv)",
   "language": "python",
   "name": "spimenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
