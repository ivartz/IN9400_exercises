{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Introduction to Pytorch\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This weekâ€™s exercise will walk you through the basics of Pytorch. The goal is for you to be familiar with:\n",
    "- What is a computational graph\n",
    "- How to define a pytorch data loader\n",
    "- How to build a neural network in pytorch\n",
    "- How to define a pytorch optimizer\n",
    "- How to train a neural network in pytorch\n",
    "\n",
    "\n",
    "In addition to this exercise, we recommend you to have a look at the official tutorials on pytorch.org\n",
    "\n",
    "https://pytorch.org/tutorials/index.html\n",
    "\n",
    "Links:\n",
    "- [Task1: Computational graph](#Task1)\n",
    "- [Task2: Play with MNIST Fashion and Pytroch](#Task2)\n",
    "\n",
    "\n",
    "\n",
    "Software verion:\n",
    "- Python 3.6\n",
    "- Pytorch 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Task1'></a>\n",
    "## Task1: Computational graph\n",
    "---\n",
    "\n",
    "Here you should compute the forward and the backward pass of the given computational graph. \n",
    "\n",
    "The graph represent the sigmoid function:     $\\sigma(\\vec{x}, \\vec{w}) = \\frac{1}{1 + exp(-[w_0x_0+w_1x_1+w_2])}$\n",
    "\n",
    "The green values are the input values which should be propagated forward, and the red values are the values you should propagate backward (the gradients). You should compute the values for the forward and backward pass for all nodes as we did in the lecture. \n",
    "\n",
    "![title](images/sigmoid_graph.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ToDo:\n",
    "# Draw by hand/on paper the computational graph of the sigmoid function and fill in the \n",
    "# values for the forward and backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "As a reference, see the following example from the lecture:\n",
    "\n",
    "<img src=\"images/simple_graph_example.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='Task2'></a>\n",
    "## Task2: Play with MNIST Fashion and Pytroch\n",
    "\n",
    "---\n",
    "Your task is to use Pytorch to build a model and train a neural network on the MNIST Fashion dataset. Before you can start you need to have access to the MNIST Fashion dataset. If you use an IFI computer, the defualt path given in this jupyter notebook will root you to the data. If you work on any other computer, you will need to download the MNIST Fashion dataset. You can download the files from: https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion\n",
    "\n",
    "MINST Fashion files:\n",
    "- t10k-images-idx3-ubyte.gz\n",
    "- t10k-labels-idx1-ubyte.gz\n",
    "- train-images-idx3-ubyte.gz\n",
    "- train-labels-idx1-ubyte.gz\n",
    "\n",
    "The MNIST Fashion dataset have 10 classes: ['T-shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'].  \n",
    "\n",
    "The training set consists of 60,000 images and the test set consists of 10,000 images. The images are of size [28,28].\n",
    "\n",
    "\n",
    "**Important!**\n",
    "You will need to add code at locations indicated with \"ToDo\" only.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>\"%matplotlib inline\"</b> is used to plot figures within the jupyter notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils.utility_functions import datasetFashionMNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Handling of the data\n",
    "\n",
    "The following cell creates two instances of \"datasetFashionMNIST\". The \"datasetFashionMNIST\" is a \"torch.utils.data.Dataset\" written for the MNIST Fashion dataset.\n",
    "\n",
    "If you do not use an IFI computer edit the \"dataPath\" to the location of the MNIST Fashion dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to the MNIST Fashion files\n",
    "#dataPath = 'data/MNIST_fashion/'\n",
    "dataPath = '/uio/kant/ifi-project03/in5400/MNIST_fashion/'\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = datasetFashionMNIST(dataPath=dataPath, train=True)\n",
    "val_dataset   = datasetFashionMNIST(dataPath=dataPath, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['T-shirt / top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "plt.figure(figsize=(18, 16), dpi=80)\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(np.array(train_dataset.labels) == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        img = (train_dataset.images[idx,:]).astype(np.uint8)\n",
    "        img = np.resize(img, (28, 28))   # reshape to 28x28\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To keep track of important parameters, we use dictionary \"config\". You should experiment with different values for the batch size, learning rate and number of epochs trained.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "          'batch_size':128,\n",
    "          'use_cuda': True,       #True=use Nvidia GPU | False use CPU\n",
    "          'log_interval':20,      #How often to dislay (batch) loss during training\n",
    "          'epochs': 20,           #Number of epochs\n",
    "          'learningRate': 0.001\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the instances of \"datasetFashionMNIST\" we can iterate through the data, however we would like to use pytorch's \"torch.utils.data.DataLoader\" class. It is convenient as it helps us with batching and shuffling of the data. It also makes it possible to use multiple CPU cores/threads to speed up data preprocessing. Your task is to instantiate two data loaders (one for each of the training and validation dataset objects), using PyTorch dataloader \"torch.utils.data.DataLoader\". Consider if you will use multiple workers and shuffling of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "\n",
    "#ToDo\n",
    "#train_loader = None\n",
    "#val_loader   = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Build the model\n",
    "\n",
    "You are now to define the network architecture. By default, the neural network is a fully connected neural network (dense neural network) with two hidden layer of size 128 and 64. We encourage you to play with the network configuration.\n",
    "\n",
    "The input has shape [batch size, 28x28]. The 28x28 image size are being concatenated in \"datasetFashionMNIST\". Try to change:\n",
    "- The number of layers\n",
    "- The size of the hidden layers\n",
    "- The activation functions\n",
    "\n",
    "\n",
    "Note that the model inherits from \"torch.nn.Module\", which requires the two class methods \"__init__\" and \"forward\". As discussed in the lecture, the former defines the layers used by the model, while the latter defines how the layers are stacked inside the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creat an instance of Model\n",
    "model = Model()\n",
    "if config['use_cuda'] == True:\n",
    "    model.to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Define optimizer and loss function\n",
    "\n",
    "Instantiate an optimizer, e.g. stochastic gradient descent, from the \"torch.optim\" module (https://pytorch.org/docs/stable/optim.html) with your model. Remember that we have defined \"learning rate\" inside the config-dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of \"torch.optim.SGD\"\n",
    "\n",
    "#ToDo\n",
    "optimizer = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here we want to define the loss function (often called criterion). As we are dealing with a classification problem, you should use the softmax cross entropy loss.\n",
    "\n",
    "Hint, have a look here: (https://pytorch.org/docs/stable/nn.html#torch-nn-functional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(prediction, labels):\n",
    "    \"\"\"Returns softmax cross entropy loss.\"\"\"\n",
    "    #ToDo\n",
    "    loss = None\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Set up the training process and train the model\n",
    "\n",
    "You have all the building blocks needed to set up the training process. You will implement the function \"run_epoch\" which shall loop though a dataset and train the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, epoch, data_loader, optimizer, is_training, config):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model        (obj): The neural network model\n",
    "        epoch        (int): The current epoch\n",
    "        data_loader  (obj): A pytorch data loader \"torch.utils.data.DataLoader\"\n",
    "        optimizer    (obj): A pytorch optimizer \"torch.optim\"\n",
    "        is_training (bool): Whether to use train (update) the model/weights or not. \n",
    "        config      (dict): Configuration parameters\n",
    "\n",
    "    Intermediate:\n",
    "        totalLoss: (float): The accumulated loss from all batches. \n",
    "                            Hint: Should be a numpy scalar and not a pytorch scalar\n",
    "\n",
    "    Returns:\n",
    "        loss_avg         (float): The average loss of the dataset\n",
    "        accuracy         (float): The average accuracy of the dataset\n",
    "        confusion_matrix (float): A 10x10 matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_training==True: \n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss        = 0 \n",
    "    correct          = 0 \n",
    "    confusion_matrix = np.zeros(shape=(10,10))\n",
    "    labels_list      = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "    for batch_idx, data_batch in enumerate(data_loader):\n",
    "        if config['use_cuda'] == True:\n",
    "            images = data_batch[0].to('cuda') # send data to GPU\n",
    "            labels = data_batch[1].to('cuda') # send data to GPU\n",
    "        else:\n",
    "            images = data_batch[0]\n",
    "            labels = data_batch[1]\n",
    "\n",
    "        if not is_training:\n",
    "            with torch.no_grad():\n",
    "                # ToDo: Forward\n",
    "                prediction = None\n",
    "                # ToDo: Compute loss\n",
    "                # Note: It can be beneficial to detach \"total_loss\" from the graph, consider convert \"total_loss\" to numpy.\n",
    "                loss        = None\n",
    "                total_loss  = None  \n",
    "            \n",
    "        elif is_training: \n",
    "            # ToDo: Forward\n",
    "            prediction =None\n",
    "            # ToDo: Compute loss\n",
    "            # Note: It can be beneficial to detach \"total_loss\" from the graph, consider convert \"total_loss\" to numpy.\n",
    "            loss        = None\n",
    "            total_loss += None \n",
    "\n",
    "            # ToDo: take a gradient update\n",
    "            \n",
    "            \n",
    "\n",
    "        # Compute the correct classification\n",
    "        predicted_label  = prediction.max(1, keepdim=True)[1][:,0]\n",
    "        correct          += predicted_label.eq(labels).cpu().sum().numpy()\n",
    "        confusion_matrix += metrics.confusion_matrix(labels.cpu().numpy(), predicted_label.cpu().numpy(), labels_list)\n",
    "\n",
    "        # Print statistics\n",
    "        batchSize = len(labels)\n",
    "        if batch_idx % config['log_interval'] == 0:\n",
    "            print(f'Epoch={epoch} | {batch_idx/len(data_loader)*100:.2f}% | loss = {loss/batchSize:.5f}')\n",
    "\n",
    "    loss_avg         = total_loss / len(data_loader)\n",
    "    accuracy         = correct / len(data_loader.dataset)\n",
    "    confusion_matrix = confusion_matrix / len(data_loader.dataset)\n",
    "\n",
    "    return loss_avg, accuracy, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Here is where the action takes place!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "train_loss = np.zeros(shape=config['epochs'])\n",
    "train_acc  = np.zeros(shape=config['epochs'])\n",
    "val_loss   = np.zeros(shape=config['epochs'])\n",
    "val_acc    = np.zeros(shape=config['epochs'])\n",
    "val_confusion_matrix   = np.zeros(shape=(10,10,config['epochs']))\n",
    "train_confusion_matrix = np.zeros(shape=(10,10,config['epochs']))\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    train_loss[epoch], train_acc[epoch], train_confusion_matrix[:,:,epoch] = \\\n",
    "                               run_epoch(model, epoch, train_loader, optimizer, is_training=True, config=config)\n",
    "\n",
    "    val_loss[epoch], val_acc[epoch], val_confusion_matrix[:,:,epoch]     = \\\n",
    "                               run_epoch(model, epoch, val_loader, optimizer, is_training=False, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 5. Show results\n",
    "Plot the loss and the accuracy as a function of epochs to monitor the training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training accuracy and the training loss\n",
    "#plt.figure()\n",
    "plt.figure(figsize=(18, 16), dpi= 80, facecolor='w', edgecolor='k')\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "# plt.subplots_adjust(hspace=2)\n",
    "ax.plot(train_loss, 'b', label='train loss')\n",
    "ax.plot(val_loss, 'r', label='validation loss')\n",
    "ax.grid()\n",
    "plt.ylabel('Loss', fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=18)\n",
    "ax.legend(loc='upper right', fontsize=16)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "ax.plot(train_acc, 'b', label='train_acc')\n",
    "ax.plot(val_acc, 'r', label='validation accuracy')\n",
    "ax.grid()\n",
    "plt.ylabel('Accuracy', fontsize=18)\n",
    "plt.xlabel('Iterations', fontsize=18)\n",
    "val_acc_max = np.max(val_acc)\n",
    "val_acc_max_ind = np.argmax(val_acc)\n",
    "plt.axvline(x=val_acc_max_ind, color='g', linestyle='--', label='Highest validation accuracy')\n",
    "plt.title('Highest validation accuracy = %0.1f %%' % (val_acc_max*100), fontsize=16)\n",
    "ax.legend(loc='lower right', fontsize=16)\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let us study the accuracy per class on the validation dataset. We use the result from the epoch with highest validation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argmax(val_acc)\n",
    "class_accuracy = val_confusion_matrix[:,:,ind]\n",
    "for ii in range(len(classes)):\n",
    "    acc = val_confusion_matrix[ii,ii,ind] / np.sum(val_confusion_matrix[ii,:,ind])\n",
    "    print(f'Accuracy of {str(classes[ii]).ljust(15)}: {acc*100:.01f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "In order to see how the network learns to distinguish the different classes as the training progresses we can plot the confusion matrices as heatmaps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "epoch_step                  = 2    \n",
    "set_colorbar_max_percentage = 10 \n",
    "    \n",
    "# Plot confusion matrices\n",
    "ticks = np.linspace(0,9,10)\n",
    "gridspec_kwargs = dict(top=0.9, bottom=0.1, left=0.0, right=0.9, wspace=0.5, hspace=0.2)\n",
    "for i in range(0, config['epochs'], epoch_step):\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 16), gridspec_kw=gridspec_kwargs)\n",
    "    im = ax1.imshow(val_confusion_matrix[:, :, i]*100)\n",
    "    ax1.set_title(f'Validation: Epoch #{i}', fontsize=18)\n",
    "    ax1.set_xticks(ticks=ticks)\n",
    "    ax1.set_yticks(ticks=ticks)\n",
    "    ax1.set_yticklabels(classes)\n",
    "    im.set_clim(0.0, set_colorbar_max_percentage)\n",
    "    ax1.set_xticklabels(classes, rotation=45)\n",
    "    ax1.set_ylabel('Prediction', fontsize=16)\n",
    "    ax1.set_xlabel('Groundtruth', fontsize=16)\n",
    "    divider = make_axes_locatable(ax1)\n",
    "    cax     = divider.append_axes('right', size='5%', pad=0.15)\n",
    "    f.colorbar(im, cax=cax, orientation='vertical')\n",
    "    \n",
    "    im = ax2.imshow(train_confusion_matrix[:, :, i]*100)\n",
    "    ax2.set_title(f'Train: Epoch #{i}', fontsize=18)\n",
    "    ax2.set_xticks(ticks=ticks)\n",
    "    ax2.set_yticks(ticks=ticks)\n",
    "    ax2.set_yticklabels(classes)\n",
    "    im.set_clim(0.0, set_colorbar_max_percentage)\n",
    "    ax2.set_xticklabels(classes, rotation=45)\n",
    "    ax2.set_ylabel('Prediction', fontsize=16)\n",
    "    ax2.set_xlabel('Ground truth', fontsize=16)\n",
    "    divider = make_axes_locatable(ax2)\n",
    "    cax     = divider.append_axes('right', size='5%', pad=0.15)\n",
    "    f.colorbar(im, cax=cax, orientation='vertical')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
