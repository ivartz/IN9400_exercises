{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradients (Reinforce)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook lets you run the reinforce algorithm on the \"CartPole_v1\" problem from Open AI gym (https://gym.openai.com/envs/CartPole-v1/).\n",
    "As this is the solution, your task is to understand the code. I suggest you start with looking at the implementation of \"Reinforce\" in the address given below. \n",
    "\n",
    "The \"CartPole_v1\" problem is solved in two versions:\n",
    "1. The observed state is a list of: [cart position, cart velocity, pole angle, pole velocity at tip]\n",
    "2. The observed state is an image of the cart\n",
    "\n",
    "To change between the two versions, select the appropriate network in the \"config\" dict. \n",
    "- 'CartPole_v1'\n",
    "- 'CartPole_v1_image'\n",
    "\n",
    "**Note:** <br/>\n",
    "The rendering function showing the cartpole will most likely not work on the ML servers.\n",
    "\n",
    "\n",
    "**Reinforce:** <br/>\n",
    "https://github.com/pytorch/examples/blob/master/reinforcement_learning/reinforce.py\n",
    "\n",
    "**Deep Q Learning (DQN):** <br/>\n",
    "For deep Q learning, look at the pytroch's official tutorial. <br/>\n",
    "https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "You can also expand the code to work on other problems. Try changing the environment within \"modelParam\". \n",
    "- Acrobot-v1\n",
    "- MountainCar-v0\n",
    "- MountainCarContinous-v0\n",
    "- Pendulum-v0\n",
    "\n",
    "Software version:\n",
    "- Python 3.6\n",
    "- Pytorch 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.saverRestorer import SaverRestorer\n",
    "from utils.model import Model\n",
    "from utils.trainer import Trainer\n",
    "from utils.player import Player\n",
    "from utils.environment import EnvironmentWrapper, EnvironmentWrapper_image\n",
    "\n",
    "def main(config, modelParam):\n",
    "    if config['network'] == 'CartPole_v1_image':\n",
    "        env = EnvironmentWrapper_image(modelParam)\n",
    "    else:\n",
    "        env = EnvironmentWrapper(modelParam)\n",
    "\n",
    "    # create an instance of the model you want\n",
    "    model = Model(config, modelParam, env)\n",
    "\n",
    "    # create an instacne of the saver and resoterer class\n",
    "    saveRestorer = SaverRestorer(config, modelParam)\n",
    "    model        = saveRestorer.restore(model)\n",
    "\n",
    "    # here you train your model\n",
    "    if modelParam['play'] == False:\n",
    "        trainer = Trainer(model, modelParam, config, saveRestorer, env)\n",
    "        trainer.train()\n",
    "\n",
    "    #play\n",
    "    if modelParam['play'] == True:\n",
    "        player = Player(model, modelParam, config, saveRestorer, env)\n",
    "        player.play_episode()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelParam = {\n",
    "    'episode_batch': 32,           # Training batch size, number of games before parameter update\n",
    "    'numb_of_updates': 100,        # Number of gradient descent updates\n",
    "    'max_episode_len': 500,        # Max number of steps before the game is terminated\n",
    "    'cuda': {'use_cuda': False,    # Use_cuda=True: use GPU\n",
    "             'device_idx': 0},     # Select gpu index: 0,1,2,3\n",
    "    'environment': 'CartPole-v1',  # Game selected\n",
    "    'modelsDir': 'storedModels/',\n",
    "    'restoreModelLast': 0,         # 0= train from scratch, 1=restore previously trained model\n",
    "    'restoreModelBest': 0,\n",
    "    'storeModelFreq': 3,           # How often you want to save your model\n",
    "    'render': False,               # True if you want to visualize the game while training\n",
    "    'is_train': True,\n",
    "    'inNotebook': True,   # If running script in jupyter notebook\n",
    "    'play': False         # False=train the model | True=restore pretrained model and play an episode\n",
    "}\n",
    "\n",
    "config = {\n",
    "    'optimizer': 'adam',           # 'SGD' | 'adam' | 'RMSprop'\n",
    "    'learningRate': {'lr': 0.01},  # learning rate to the optimizer\n",
    "    'weight_decay': 0,             # weight_decay value\n",
    "    'gamma': 0.99,                 # discount factor\n",
    "    'seed': 543,\n",
    "    'network': 'CartPole_v1'       # 'CartPole_v1' | 'CartPole_v1_image'\n",
    "}\n",
    "\n",
    "if modelParam['play'] == True:\n",
    "    modelParam['restoreModelLast'] = 1\n",
    "    modelParam['render'] = True\n",
    "\n",
    "main(config, modelParam)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
